# -*- coding: utf-8 -*-
"""Predict the House Prices using  Artificial Neural Network Regression method.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFvMDFkM6HuQXenU3-zuSKfKF5U6xt58

##STEP#0: Import Libraries
"""

#!pip install tensorflow-gpu==2.0.0.alpha0

!pip uninstall -y tensorflow
!pip install tensorflow

import tensorflow as tf
import pandas as pd
import seaborn as sns  # to plot images and statical version of data
import numpy as np
import matplotlib.pyplot as plt

"""## STEP #1: Importing Dataset"""

from google.colab import drive
drive.mount('/content/drive')

house_sale = pd.read_csv('/content/drive/My Drive/ML_Dataset/House Sale Prediction/kc_house_data.csv')

house_sale

house_sale.head()

house_sale.tail()

house_sale.info()

house_sale.describe()

"""## STEP #2: Visualize the Dataset"""

plt.scatter(house_sale['sqft_living'], house_sale['price'])
plt.xlabel('sqft_living')
plt.ylabel('price')
plt.show()

sns.scatterplot(x= 'sqft_living', y= 'price', data=house_sale)
plt.xlabel('sqft_living')
plt.ylabel('price')
plt.show()

# looking into the correlation between the independent variable
f, ax = plt.subplots(figsize = (30, 30))
sns.heatmap(house_sale.corr(), annot= True)

# plotting histogram of the data (distribution of all the columns)
house_sale.hist(figsize = (20,20))

sns.pairplot(house_sale)



"""## STEP #3 Creating Training & Testing Dataset"""

X = house_sale[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement']]

X

y = house_sale['price']

y

X.shape

y.shape

# the output data is not normalised, so making it normalized usign skit learn
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

X_scaled.shape

scaler.data_max_

scaler.data_min_

y = y.values.reshape(-1,1)

y_scaled = scaler.fit_transform(y)

y_scaled



"""## STEP #4: Training the model"""

# spilitting data into training and testing data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size = 0.75)

X_train.shape

X_test.shape

# in tensorflow 2.0, keras API come automatically with tensorflow 2.0
ANN_model = tf.keras.models.Sequential()
ANN_model.add(tf.keras.layers.Dense(units = 100, activation = 'relu', input_shape = (7,))) #we use Dense when we have fully connected atificial neural network
# now we are adding one more layer to the network
ANN_model.add(tf.keras.layers.Dense(units = 100, activation = 'relu'))
ANN_model.add(tf.keras.layers.Dense(units = 100, activation = 'relu'))
# now adding the output layer
ANN_model.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))

ANN_model.summary()

ANN_model.compile(optimizer = 'Adam', loss= 'mean_squared_error')

# Splitting training dataset with validation, so that model can preditc validation dataset simultaneously as well
hist = ANN_model.fit(X_train, y_train, epochs = 100, batch_size=50, validation_split = 0.2)



"""## STEP #5: Evaluating the Model"""

hist.history.keys()

plt.plot(hist.history['loss'], color='g', label='Training loss')
plt.plot(hist.history['val_loss'], color='b', label='Validation loss')
plt.title('Model loss progress during training the model')
plt.xlabel('Epochs')
plt.ylabel('Training loss & Validation Loss')
plt.legend()
plt.show()

#  'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement'
X_test_1 = np.array([[4, 3, 1960, 5000, 1, 2000, 3000]])

scaler_1 = MinMaxScaler()
X_test_scaled_1 = scaler_1.fit_transform(X_test_1)

y_predict_1 = model.predict(X_test_scaled_1) #predicting the values

y_predict_1 = scaler.inverse_transform(y_predict_1)
y_predict_1

y_predict = model.predict(X_test) #predicting the values
plt.plot(y_test, y_predict, '*', color = 'r')
plt.xlabel('Model Prediction')
plt.ylabel('True Values')
plt.show()

# Taking Original Values as it was in the data set

y_predict_orig = scaler.inverse_transform(y_predict)

# Taking Original Values as it was in the data set
y_test_orig = scaler.inverse_transform(y_test)

plt.plot(y_test_orig, y_predict_orig, '+', color = 'b')
plt.xlabel('Model Prediction')
plt.ylabel('True Values')
plt.show()

# Now visualising the Model performance with different error type
k = X_test.shape[1]

k

n = len(X_test)
n

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)), '0.3f'))

RMSE

MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =', RMSE, '\nMSE=', MSE, '\nMAE=', MAE, '\nR2=', r2, '\nAdjusted R2=', adj_r2)



"""## Lets increase the number of feature (independent variables) and retrain the exact same model"""

house_sale

X_extended = house_sale.drop(['id', 'date', 'price'], axis=1)

X_extended

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_extended)

y = house_sale['price']

y = y.values.reshape(-1,1)

y_scaled = scaler.fit_transform(y)

X_scaled.shape

# spilitting data into training and testing data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size = 0.75)

# in tensorflow 2.0, keras API come automatically with tensorflow 2.0
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units = 100, activation = 'relu', input_shape = (18,))) #we use Dense when we have fully connected atificial neural network
# now we are adding one more layer to the network
model.add(tf.keras.layers.Dense(units = 100, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 100, activation = 'relu'))
# now adding the output layer
model.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))

model.summary()

model.compile(optimizer = 'Adam', loss= 'mean_squared_error')

# Splitting training dataset with validation, so that model can preditc validation dataset simultaneously as well
hist = model.fit(X_train, y_train, epochs = 100, batch_size=50, validation_split = 0.2)

hist.history.keys()

plt.plot(hist.history['loss'], color='g', label='Training loss')
plt.plot(hist.history['val_loss'], color='b', label='Validation loss')
plt.title('Model loss progress during training the model')
plt.xlabel('Epochs')
plt.ylabel('Training loss & Validation Loss')
plt.legend()
plt.show()



y_predict = model.predict(X_test) #predicting the values
plt.plot(y_test, y_predict, '*', color = 'r')
plt.xlabel('Model Prediction')
plt.ylabel('True Values')
plt.show()

# Taking Original Values as it was in the data set

y_predict_orig = scaler.inverse_transform(y_predict)

# Taking Original Values as it was in the data set
y_test_orig = scaler.inverse_transform(y_test)

plt.plot(y_test_orig, y_predict_orig, '+', color = 'b')
plt.xlabel('Model Prediction')
plt.ylabel('True Values')
plt.show()

k = X_test.shape[1]
k

n = len(X_test)
n

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)), '0.3f'))

RMSE

MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =', RMSE, '\nMSE=', MSE, '\nMAE=', MAE, '\nR2=', r2, '\nAdjusted R2=', adj_r2)





